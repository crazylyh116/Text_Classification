# BERT

## 1. Principle





## 2. Implemented by PyTorch:TODO

Please refer to this repo: [Transformers_for_Text_Classification](https://github.com/zhanlaoban/Transformers_for_Text_Classification).




## 3. Implemented by TensorFlow: TODO





# Collections of Text Classification

## PyTorch

1. [Bert-Pytorch-Chinese-TextClassification](https://github.com/xieyufei1993/Bert-Pytorch-Chinese-TextClassification)

   > 提供完数据集和步骤，代码可以跑通，但仅能进行训练与验证，没有测试的代码

2. [Bert-Chinese-Text-Classification-Pytorch](https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch)

   > 支持训练、验证与测试(该测试也是验证，还是不能进行预测)，拥有完整数据集
   >
   > 支持BERT、ERNIE
   >
   > 支持BERT与CNN、RNN、RCNN和DPCNN的结合使用

## TensorFlow

1. [BERT-chinese-text-classification-and-deployment](https://github.com/SunYanCN/BERT-chinese-text-classification-and-deployment)

   > 可以训练、验证与测试，支持使用**TensorFlow Serving**进行部署
   
2. [TextClassify_with_BERT](https://github.com/yaleimeng/TextClassify_with_BERT)

   > 支持预测。提供完整数据集。可以使用TensorFlow Serving进行部署。



# Others Pretrained Model

1. BERT-wwm

   > 1. [Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/pdf/1906.08101.pdf)
   >
   > 2. [中文预训练BERT-wwm（Pre-Trained Chinese BERT with Whole Word Masking）](https://github.com/ymcui/Chinese-BERT-wwm)
   >
   >       基于全词遮掩（Whole Word Masking）技术的中文预训练模型BERT-wwm

2. RoBERTa

   > 1. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
   >
   > 2. [RoBERTa中文预训练模型: RoBERTa for Chinese](https://github.com/brightmart/roberta_zh)
   >
   >       RoBERTa for Chinese, TensorFlow & PyTorch
   
3. XLNet

   > 1. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)
   > 2. [Pre-Trained Chinese XLNet（中文XLNet预训练模型）](https://github.com/ymcui/Chinese-PreTrained-XLNet)
   > 3. [中文预训练XLNet模型: Pre-Trained Chinese XLNet_Large](https://github.com/brightmart/xlnet_zh)

4. ERNIE

   > 1. [ERNIE](https://github.com/PaddlePaddle/ERNIE)
   >
   > 2. [ERNIE-Pytorch](https://github.com/nghuyong/ERNIE-Pytorch)
   >
   >    ERNIE Pytorch Version

5. GPT-2

   > 1. [Chinese version of GPT2 training code, using BERT or BPE tokenizer.](https://github.com/Morizeyao/GPT2-Chinese)

6. Others

   [多领域开源中文预训练语言模型仓库](https://github.com/thunlp/OpenCLaP)



# Related Nice Blogs

1. [nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493)