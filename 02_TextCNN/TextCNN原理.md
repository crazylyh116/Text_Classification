# 一、卷积神经网络原理

## 卷积神经网络的特点：

#### 1. 稀疏交互(sparse interactions): 也叫稀疏权重(sparse weights)、稀疏连接(sparse connectivity)

在传统神经网络中，网络层之间输入与输出的连接关系可以由一个权值参数矩阵来表示。对于全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构。这里面的**交互**是指每个单独的参数值，该参数值表示了前后层某两个神经元节点之间的交互。

在卷积神经网络中，**卷积核尺度远小于输入的维度**，这样**每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重（即产生交互）**，我们称这种特性为**稀疏交互**。

稀疏交互的**物理意义**：通常图像、文本、语音等现实世界中的数据都具有**局部的特征结构**， 我们可以先学习局部的特征， 再将局部的特征组合起来形成更复杂和抽象的特征。

#### 2. 参数共享(parameter sharing)

参数共享是指在同一个模型的不同模块中使用相同的参数。卷积运算中的参数共享让网络只需要学一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。

参数共享的**物理意义**：使得卷积层具有平移等变性。在第三个特点中会谈到。

显然，我们可以看到，卷积神经网络在存储大小和统计效率方面极大地优于传统的使用矩阵乘法的神经网络。

#### 3. 等变表示(equivariant representations)

假如图像中有一只猫，那么无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。特别地，当函数f(x)与g(x)满足f(g(x))=g(f(x))时，我们称f(x)关于变换g具有等变性。在猫的图片上先进行卷积，再向右平移l像素的输出，与先将图片向右平移l像素再进行卷积操作的输出结果是相等的。

 

## 卷积操作

### 1. 一维卷积

 

### 2. 二维卷积

### 3. 三维卷积

 

## 池化操作

池化操作主要针对非重叠区域，包括均值池化（mean pooling）、最大池化（max pooling）。

**均值池化**通过对邻域内特征数值求平均来实现，能够抑制由于邻域大小受限造成估计值方差增大的现象，特点是对背景的保留效果更好。均值池化是求窗口中元素的均值。

**最大池化**则通过取邻域内特征的最大值来实现，能够抑制网络参数误差造成估计均值偏移的现象，特点是更好地提取纹理信息。最大池化则求窗口中元素的最大值 。

池化操作的本质是**降采样**。例如，我们可以利用最大池化将4×4的矩阵降采样为2×2的矩阵。

特殊的池化方式还包括**相邻重叠区域的池化**以及**空间金字塔池化**。池化操作除了能显著降低参数量外，还能够保持对平移、伸缩、旋转操作的**不变性**。这里就不展开细讲了。

 

 

# 二、卷积神经网络在文本分类上的应用

相关论文：[Convolutional Neural Networks for Sentence Classification](Convolutional Neural Networks for Sentence Classification)

卷积神经网络的核心思想是**捕捉局部特征**，对于文本来说，局部特征就是**由若干单词组成的滑动窗口**，类似于N-gram。卷积神经网络的优势在于能够**自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息**。

图1是论文Convolutional Neural Networks for Sentence Classification中用于文本分类的卷积神经网络模型架构。

![2019-07-22_022137.png](https://i.loli.net/2019/07/22/5d351f31c787e77342.png)

<center>图1 文本分类CNN架构</center>
（1）输入层是一个N×K的矩阵，其中N为文章所对应的单词总数，K是每个词对应的表示向量的维度。也就是说，输入层的每一行就是一个词向量，词向量为K维，如第一行为单词wait的词向量，第二行为for的词向量。每个词向量可以是预先在其他语料库中训练好的，也可以作为未知的参数由网络训练得到。这两种方法各有优势，**预先训练的词嵌入可以利用其他语料库得到更多的先验知识**，而**由当前网络训练的词向量能够更好地抓住与当前任务相关联的特征**。因此，图中的输入层实际采用了**两个通道**的形式，即有两个N×K的输入矩阵，其中**一个用预先训练好的词嵌入表达，并且在训练过程中不再发生变化**；**另外一个也由同样的方式初始化，但是会作为参数，随着网络的训练过程发生改变**。

（2） 第二层为卷积层。在输入的N×K维矩阵上，我们定义不同大小的滑动窗口进行卷积操作
$$
c_{i}=f\left(w \cdot x_{i : i+h-1}+b\right)
$$
其中$x_{i : i+h-1}$代表由输入矩阵的第i行到第i+h-1行所组成的一个大小为h×K的**滑动窗口**，w为K×h维的**权重矩阵**，b为**偏置参数**。假设h为3，则每次在2×K的滑动窗口上进行卷积，并得到N-2个结果，再将这N-2个结果拼接起来得到N-2维的特征向量。**每一次卷积操作相当于一次特征向量的提取，通过定义不同的滑动窗口，就可以提取出不同的特征向量，构成卷积层的输出。**

（3）第三层为池化层，比如图中所示的网络采用了**1-Max池化**，即为从每个滑动窗口产生的特征向量中筛选出一个最大的特征，然后将这些特征拼接起来构成向量表示。也可以选用**K-Max池化**（选出每个特征向量中最大的K个特征），或者平均池化（将特征向量中的每一维取平均）等，达到的效果**都是将不同长度的句子通过池化得到一个定长的向量表示。**

（4）得到文本句子的向量表示之后，后面的网络结构就和具体的任务相关了。本例中展示的是一个文本分类的场景，因此最后接入了一个全连接层，并使用Softmax激活函数输出每个类别的概率。

 

 

 

 

 

 

 

 

 

 

 

# 三、其他相关问题

### 1. 训练时是否可以将神经网络的全部参数初始化为0 ？

### 2. Dropout是如何起作用的？

#### 原理

#### 实现

#### 3. 批量归一化(Batch Batch Normalization)的原理是什么？

#### 4. 调参Tricks：

[A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1510.03820)

 
